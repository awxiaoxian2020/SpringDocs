[[designing-twitter]]
== Designing Twitter

设计一个类似Twitter的网络社交服务。服务中的用户可以发布推文、关注其他用户和收藏推文。难度等级：中等

[[twitter是什么]]
=== 1.Twitter是什么？

Twitter是一款在线社交网络服务，用户可以发布和阅读称之为“推文”的140字符的消息。注册的用户可以发布和阅读推文，然而未注册的用户只能阅读推文。用户通过网站界面、SMS或者手机应用程序访问Twitter。

[[需求和系统目标]]
=== 2. 需求和系统目标

我们将使用以下需求设计一个简单版本的Twitter：

功能性需求: ::

. 用户应该可以发布新的推文；
. 用户可以关注其他用户；
. 用户可以收藏推文；
. 系统可以提创建并显示用户的时间线，由用户关注的其他人的发布的推文组成，按时间顺序排列；
. 推文可以包含照片和视频。

非功能需求: ::

. 服务需要高可用；
. 系统时间线生成的可接受的延迟是200毫秒；
. 可以达到数据一致性（在牺牲可用性的条件下）；如果用户一段时间内看不到某条推文，这是可以接受的。

扩展的需求: ::

. 可以搜索推文；
. 可以在推文下回复消息；
. 热门话题 - 当前的热点话题/搜索最多的话题
. 给其他用户打标签；
. 推文通知；
. 谁关注了你？进行内容推荐？
. 精彩瞬间。

[[能力评估和约束]]
=== 3. 能力评估和约束

假设我们总共有10亿用户，日活跃用户（DAU）有2亿。也假设每天有1亿新的推文，并且平均每个用户关注200个其他用户。

一天有多少个收藏？如果，平均每个用户每天收藏5个推文，那么将会有：

[source,text]
----
200M users * 5 favorites => 1B favorites
----

我们的系统总共会生成多少推文页面？假设平均一个用户每天访问他们的时间线主页两次，并且访问其他人的主页5次。如果一个用户在每个页面上看到20个推文，那么系统每天浏览推文的数量是20B：

[source,text]
----
200M DAU * ((2 + 5) * 20 tweets) => 28B/day
----

存储评估假设每条推文有140个字符，我们需要两个字节存储一个没有被压缩的字符。假设我们需要30个字节存储每条推文的元数据（比如ID，时间戳，用户ID等）。总计需要存储量是：

[source,text]
----
100M * (280 + 30) bytes => 30GB/day
----

5的年数据需要多大的存储量呢？ 需要多少空间存储用户的数据、关注数据和收藏数据？将这些留给读者作为练习。

不是每条推文都有媒体数据，假设平均每５条推文中有一张图片，每１０条推文中有一个视频。假设平均每张图片的大小是200KB，每个视频的大小是2MB。那么每天将会有24TB的媒体数据。

[source,text]
----
(100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day
----

*带宽预估* 因为每天有24TB的数据，因此数据传输速度是290MB/秒。请记住，我们每天有280亿的推文浏览量。我们必须展示每个推文中的图片，但是假设用户平均观看到他们时间轴上的第三个视频。因此，总输出数据量是： *Bandwidth Estimates* Since total ingress is 24TB per day, this would translate into 290MB/sec.

[source,text]
----
(28B * 280 bytes) / 86400s of text => 93MB/s + (28B/5 * 200KB ) / 86400s of photos => 13GB/S + (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s
Total ~= 35GB/s
----

[[system-APIs]]
=== 4. 系统的APIs

[NOTE]
一旦我们明确了需求，那么定义系统的API接口是一个很好的主意。这应该明确说明期望系统提供的功能。

我们可以使用SOAP或者Rest API来暴露我们服务提供的功能。以下定义的是发布一个新推文接口：

[source,text]
----
tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids, maximum_results_to_return)
----

*参数：*

. api_dev_key (string): 注册账户的API开发者密钥。这样用于根据分配的配额限制用户。
. tweet_data (string): 推文的内容，通常最多为140个字符。
. tweet_location (string): 此推文所引用的可选的位置（经度，纬度）。
. user_location (string): 用户添加到推文的可选的位置（经度，纬度）。
. media_ids (number[]): 可选的推文相关的媒体数据ID的列表。（需要单独上传所有媒体照片、视频）。

*返回：* （字符串）成功地发布推文后返回访问此推文的URL地址。否则，返回一个合适的HTTP错误。

[[high-level-system-design]]
=== 5. 高级系统设计

我们需要一个可以高效地存储所有新的推文的系统，每秒存储100M/86400s => 1150个推文，并且每秒可读取28B/86400s => 325K个推文。从需求中可以看出，这将是一个读取密集型的系统。

在高层级上，我们需要多个应用服务器来处理所有的请求，并在它们前面使用负载均衡器进行流量分配。在后端，我们需要一个高效的数据库来存储所有新推文，并支持大量的读操作。我们也需要一些文件存储来保存图片和视频。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/twitter/design-twitter-high-level-system-design.png[]

尽管我们预期的写入量是每天1亿条推文，读取量是每日280亿推文。这意味着我们的系统平均每秒将接受1160条新的推文的写入和325000条读取推文的操作。该流量在一天中分布不均匀，但是在高峰期，我们应该预计每秒至少有几千条推文的写入请求和每秒大约1,000,000条读取推文的请求。在设计系统架构时，我们应该牢记这一点。

[[database-schema]]
=== 6. 数据库架构

我们需要存储的数据有用户的信息、用户的推文、用户收藏的推文和用户关注的其他用户的信息。

.Tweet
[width="25%",cols="<s,>m"]
|===
2+| Paste
|PK|TweetID: int
||Content: varchar(140)
||TweetLongitude: int
||UserLongitude: int
||CreationDate:datetime
||NumFavorites: int
|===

.User
[width="25%",cols="<s,>m"]
|===
2+| User
|PK|UserID:int
||Name:varchar(20)
||Email:varchar(32)
||DateOfBirth:datetime
||CreationDate:datetime
||LastLogin:datetime
|===

.UserFollow
[width="25%",cols="<s,>m"]
|===
2+| UserFollow
|PK|[Not supported by viewer]
|===

.Favorite
[width="25%",cols="<s,>m"]
|===
2+|Favorite
|Pk| [Not supported by viewer]
||CreationDate:datetime|
|===

对于选择使用关系型数据库还是非关系型数据库来存储数据，请参考设计Instagram中的 link:designing-instagram.adoc#数据库架构[数据库架构] 。

[[data-sharding]]
=== 7. 数据分片

由于我们每天都有大量新的推文，而且我们的读取负载也非常高，我们需要将数据分布到多台机器上，以便我们可以高效地读取/写数据。 我们有很多选择来分片我们的数据； 让我们逐个介绍：

*基于UserID的分片：* 我们可以尝试将同一用户的所有数据存储在一台服务器上。 在存储时，我们可以将 UserID 传递给哈希函数，该函数将用户映射到数据库服务器，我们将在其中存储用户的所有推文、收藏夹、关注等信息。 在查询用户的推文/关注/收藏时，我们可以询问哈希函数在哪一个数据库可以找到用户的数据，然后从那里读取。这种方法有几个问题：

. 如果用户变成热点数据怎么办？ 存储此用户数据的服务器上可能有很多查询。这种高负载会影响我们服务的性能。
. 随着时间的推移，一些用户最终可能会存储大量推文或与其他用户相比拥有大量关注。 保持不断增长的用户数据的均匀分布是相当困难的。

为了解决这些问题，我们必须重新分区或重新分配我们的数据，或者使用一致性哈希函数来解决这些问题。

*基于TweetID的分区:* 哈希函数会将TweetID映射到一个随机的服务器，我们将在其中存储该推文。我们必须查询所有服务器来搜索推文，每个服务器都会返回一组推文。中央服务器将汇总这些结果并将它们返回给用户。让我们看一下时间线生成示例； 以下是我们的系统为生成用户的时间线而必须执行的步骤数：

. 我们的应用（app）服务器会找到用户关注的所有人；
. 应用服务器会将查询发送到所有数据库服务器以查找来自这些人的推文；
. 每个数据库服务器都会找到每个用户的推文，按新近度对它们进行排序并返回排名靠前的推文；
. 应用服务器会将所有结果合并并重新排序，将排名靠前的结果返回给用户。

这种方法解决了热点用户的问题，但是，与通过 UserID 分片相比，我们必须查询所有数据库分区才能找到用户的推文，这会导致高延迟。

我们可以通过在数据库服务器前引入缓存来存储热门推文来进一步提高服务器的性能。


*基于推文创建时间分区：* 根据创建时间存储推文将使我们能够快速获取所有热门推文，并且我们只需要查询非常小的一组服务器。这里的问题是流量负载不会被分配，例如，在写入时，所有新推文都将发送到一台服务器，而其余服务器将处于空闲状态。同样，在读取时，与保存旧数据的服务器相比，保存最新数据的服务器将具有非常高的负载。

*如果我们可以组合 TweedID 和 Tweet 创建时间的分片呢？* 如果我们同时使用存储推文的创建时间和 TweetID 来反映这一点，我们可以获得这两种方法的好处。这样可以很快找到最新的推文。为此，我们必须使每个 TweetID 在系统中是唯一的，并且每个 TweetID 也应该包含一个时间戳。

我们可以使用纪元时间。假设我们的 TweetID 将有两部分：第一部分将表示纪元秒数，第二部分将是一个自动递增的序列。
因此，要创建一个新的 TweetID，我们可以采用当前纪元时间并为其附加一个自动递增的数字。
我们可以从这个 TweetID 中找出分片号并将其存储在相应的服务器。

TweetID 的大小可能是多少？
假设我们的纪元时间从今天开始，我们需要多少位来存储未来 50 年的秒数数据？

[source,text]
----
86400 sec/day * 365 (days a year) * 50 (years) => 1.6B
----

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/twitter/design-twitter-data-sharding.png[]

We would need 31 bits to store this number.
Since on average we are expecting 1150 new tweets per second, we can allocate 17 bits to store auto incremented sequence; this will make our TweetID 48 bits long.
So, every second we can store (2^17 => 130K) new tweets.
We can reset our auto incrementing sequence every second.
For fault tolerance and better performance, we can have two database servers to generate auto-incrementing keys for us, one generating even numbered keys and the other generating odd numbered keys.

If we assume our current epoch seconds are “1483228800,” our TweetID will look like this:

[source,text]
----
1483228800 000001
1483228800 000002
1483228800 000003
1483228800 000004
…
----

If we make our TweetID 64bits (8 bytes) long, we can easily store tweets for the next 100 years and also store them for mili-seconds granularity.

In the above approach, we still have to query all the servers for timeline generation, but our reads (and writes) will be substantially quicker.

. Since we don’t have any secondary index (on creation time) this will reduce our write latency.
. While reading, we don’t need to filter on creation-time as our primary key has epoch time included in it.

[[cache]]
=== 8. Cache

We can introduce a cache for database servers to cache hot tweets and users.
We can use an off-the- shelf solution like Memcache that can store the whole tweet objects.
Application servers, before hitting database, can quickly check if the cache has desired tweets.
Based on clients’ usage patterns we can determine how many cache servers we need.

*Which cache replacement policy would best fit our needs?* When the cache is full and we want to replace a tweet with a newer/hotter tweet, how would we choose?
Least Recently Used (LRU) can be a reasonable policy for our system.
Under this policy, we discard the least recently viewed tweet first.

*How can we have a more intelligent cache?* If we go with 80-20 rule, that is 20% of tweets generating 80% of read traffic which means that certain tweets are so popular that a majority of people read them.
This dictates that we can try to cache 20% of daily read volume from each shard.

*What if we cache the latest data?* Our service can benefit from this approach.
Let’s say if 80% of our users see tweets from the past three days only; we can try to cache all the tweets from the past three days.
Let’s say we have dedicated cache servers that cache all the tweets from all the users from the past three days.
As estimated above, we are getting 100 million new tweets or 30GB of new data every day (without photos and videos).
If we want to store all the tweets from last three days, we will need less than 100GB of memory.
This data can easily fit into one server, but we should replicate it onto multiple servers to distribute all the read traffic to reduce the load on cache servers.
So whenever we are generating a user’s timeline, we can ask the cache servers if they have all the recent tweets for that user.
If yes, we can simply return all the data from the cache.
If we don’t have enough tweets in the cache, we have to query the backend server to fetch that data.
On a similar design, we can try caching photos and videos from the last three days.

Our cache would be like a hash table where ‘key’ would be ‘OwnerID’ and ‘value’ would be a doubly linked list containing all the tweets from that user in the past three days.
Since we want to retrieve the most recent data first, we can always insert new tweets at the head of the linked list, which means all the older tweets will be near the tail of the linked list.
Therefore, we can remove tweets from the tail to make space for newer tweets.

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/twitter/design-twitter-cache.png[]

[[timeline-generation]]
=== 9. Timeline Generation

For a detailed discussion about timeline generation, take a look at Designing Facebook’s Newsfeed.

[[replication-and-fault-tolerance]]
=== 10. Replication and Fault Tolerance

Since our system is read-heavy, we can have multiple secondary database servers for each DB partition.
Secondary servers will be used for read traffic only.
All writes will first go to the primary server and then will be replicated to secondary servers.
This scheme will also give us fault tolerance, since whenever the primary server goes down we can failover to a secondary server.

[[load-balancing]]
=== 11. Load Balancing

We can add Load balancing layer at three places in our system 1) Between Clients and Application servers 2) Between Application servers and database replication servers and 3) Between Aggregation servers and Cache server.
Initially, a simple Round Robin approach can be adopted; that distributes incoming requests equally among servers.
This LB is simple to implement and does not introduce any overhead.
Another benefit of this approach is that if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it.
A problem with Round Robin LB is that it won’t take servers load into consideration.
If a server is overloaded or slow, the LB will not stop sending new requests to that server.
To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that.

[[monitoring]]
=== 12. Monitoring

Having the ability to monitor our systems is crucial.
We should constantly collect data to get an instant insight into how our system is doing.
We can collect following metrics/counters to get an understanding of the performance of our service:

. New tweets per day/second, what is the daily peak?
. Timeline delivery stats, how many tweets per day/second our service is delivering.
. Average latency that is seen by the user to refresh timeline.

By monitoring these counters, we will realize if we need more replication, load balancing, or caching.

[[extended-requirements]]
=== 13. Extended Requirements

*How do we serve feeds?* Get all the latest tweets from the people someone follows and merge/sort them by time.
Use pagination to fetch/show tweets.
Only fetch top N tweets from all the people someone follows.
This N will depend on the client’s Viewport, since on a mobile we show fewer tweets compared to a Web client.
We can also cache next top tweets to speed things up.

Alternately, we can pre-generate the feed to improve efficiency; for details please see ‘Ranking and timeline generation’ under link:designing-instagram.adoc[Designing Instagram].

*Retweet:* With each Tweet object in the database, we can store the ID of the original Tweet and not store any contents on this retweet object.

*Trending Topics:* We can cache most frequently occurring hashtags or search queries in the last N seconds and keep updating them after every M seconds.
We can rank trending topics based on the frequency of tweets or search queries or retweets or likes.
We can give more weight to topics which are shown to more people.

*Who to follow?How to give suggestions?* This feature will improve user engagement.
We can suggest friends of people someone follows.
We can go two or three levels down to find famous people for the suggestions.
We can give preference to people with more followers.

As only a few suggestions can be made at any time, use Machine Learning (ML) to shuffle and re- prioritize.
ML signals could include people with recently increased follow-ship, common followers if the other person is following this user, common location or interests, etc.

*Moments:* Get top news for different websites for past 1 or 2 hours, figure out related tweets, prioritize them, categorize them (news, support, financial, entertainment, etc.) using ML – supervised learning or Clustering.
Then we can show these articles as trending topics in Moments.

*Search:* Search involves Indexing, Ranking, and Retrieval of tweets.
A similar solution is discussed in our next problem  link:designing-twitter.adoc[Design Twitter Search].
