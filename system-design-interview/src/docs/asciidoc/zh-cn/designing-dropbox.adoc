== Designing Dropbox

让我们设计一个类似Dropbox或者Google Drive的文件主机服务。云端文件存储使用户可以将数据存储到远程服务器中。通常，这些服务器是由云端存储的提供者来维护的，并且使用户通过网路来使用服务器（通常是互联网）。用户以月付的方式为他们使用的云数据存储付费。类似的云服务还有：OneDrive、Google Drive Difficulty Level: Medium。

=== 1.Why Cloud Storage?

近来，因为云文件存储简化了多个设备之间数字资源的存储和交换，因此它变得非常流行。人们认为，从使用单个个人计算机到随时随地使用具有不同平台和操作系统的多个设备（例如智能手机和平板电脑）的转变，是云存储服务普及的原因。以下是此类云服务的主要优点：

*有效性：* 云存储的目标是使数据随时随地都可用。用户可以随时随地从任何设备上访问他们的文件或照片。

*可靠性和永久性：* 云存储的另一个优点是，它提供了100%的数据可靠性和持久性。云存储通过将数据备份到不同地区的服务器上来保证用户的数据不会丢失。

*可扩展性：* 用户不必担心数据的存储空间不足。在云存储中，只要你愿意支付相关的费用，那么你就可以无限制的存储数据。

假如在此之前你还没有使用过http://dropbox.com/[dropbox.com]，极力推荐你去创建一个账户，并且上传或编辑文件，并浏览它们提供的其他操作选项。这将有助于你理解本章内容。

=== 2.系统的要求和目标

[NOTE]
在面试开始之前，你应该始终明确你的需求。确保提出的问题是在面试者所想到的系统范围之内。

我们希望云存储系统实现什么？以下是我们系统的基本要求：

. 用户应该可以在任意设备上上传和下载他们的文件或者图片;
. 用户应该可以分享文件或者目录给其他用户；
. 我们的服务应该支持设备之间的数据自动同步，例如，在某一个设备上更新文件之后，它会被自动同步到所有的设备上;
. 系统应该支持GB级别大小的文件存储;
. 必须支持ACID，所有的文件操作应该保证原子性、一致性、隔离性和持久化;
. 我们的系统必须支持离线编辑。用户应该可以在离线状态下新增、删除和修改文件，并且一旦他们上线，他们所做的更改应该被同步到远程服务器和其他在线设备.

**扩展要求 **::
* 系统应该支持数据快照，以便用户可以回滚到文件的任意版本。

=== 3.设计注意事项

* 应该支撑超大的读写请求。 读写比应该是相近的。
* 在内部，文件可以存储在小块区域或者块中（比如4MB）；这带来很多好处，比如只需对文件更小部分重试失败的操作。如果用户上传文件失败，那么只需对失败的块重新上传文件。
* 我们可以通过传输修改后的块来减少数据交换量；
* 通过删除重复的块，可以节省存储空间和带宽；
* 在客户端保存元数据的备份（比如文件名，文件大小等），可以节省和客户端的通信次数；
* 对于较小的更改，客户端可以聪明地上传修改的部分，而不是上传整个数据块。

=== 4.容量估计和约束

* 假设总共有500,000,000用户，并且一亿日活跃用户（DAU）；
* 假设平均每个用户连接三个不同的设备；
* 平均来说，如果每个用户有200个文件或照片，那么总计有一千亿个文件；
* 假设文件的平均大小是100KB，那么总存储的大小是10千兆字节：
+
[source,text]
----
 100B * 100KB => 10PB
----

* 假设每分钟有一百万活跃用户。

=== 5.High Level Design

The user will specify a folder as the workspace on their device.
Any file/photo/folder placed in this folder will be uploaded to the cloud, and whenever a file is modified or deleted, it will be reflected in the same way in the cloud storage.
The user can specify similar workspaces on all their devices and any modification done on one device will be propagated to all other devices to have the same view of the workspace everywhere.

At a high level, we need to store files and their metadata information like File Name, File Size, Directory, etc., and who this file is shared with.
So, we need some servers that can help the clients to upload/download files to Cloud Storage and some servers that can facilitate updating metadata about files and users.
We also need some mechanism to notify all clients whenever an update happens so they can synchronize their files.

As shown in the diagram below, Block servers will work with the clients to upload/download files from cloud storage and Metadata servers will keep metadata of files updated in a SQL or NoSQL database.
Synchronization servers will handle the workflow of notifying all clients about different changes for synchronization.

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_5.png[title='High level design for Dropbox']

=== 6.Component Design

Let’s go through the major components of our system one by one:

*a.Client*

The Client Application monitors the workspace folder on the user’s machine and syncs all files/folders in it with the remote Cloud Storage.
The client application will work with the storage servers to upload, download, and modify actual files to backend Cloud Storage.
The client also interacts with the remote Synchronization Service to handle any file metadata updates, e.g., change in the file name, size, modification date, etc.

Here are some of the essential operations for the client:

. Upload and download files.
. Detect file changes in the workspace folder.
. Handle conflict due to offline or concurrent updates.

*How do we handle file transfer efficiently?* As mentioned above, we can break each file into smaller chunks so that we transfer only those chunks that are modified and not the whole file.
Let’s say we divide each file into fixed sizes of 4MB chunks.
We can statically calculate what could be an optimal chunk size based on 1) Storage devices we use in the cloud to optimize space utilization and input/output operations per second (IOPS) 2) Network bandwidth 3) Average file size in the storage etc.
In our metadata, we should also keep a record of each file and the chunks that constitute it.

*Should we keep a copy of metadata with Client?* Keeping a local copy of metadata not only enable us to do offline updates but also saves a lot of round trips to update remote metadata.

*How can clients efficiently listen to changes happening with other clients?* One solution could be that the clients periodically check with the server if there are any changes.
The problem with this approach is that we will have a delay in reflecting changes locally as clients will be checking for changes periodically compared to a server notifying whenever there is some change.
If the client frequently checks the server for changes, it will not only be wasting bandwidth, as the server has to return an empty response most of the time, but will also be keeping the server busy.
Pulling information in this manner is not scalable.

A solution to the above problem could be to use HTTP long polling.
With long polling the client requests information from the server with the expectation that the server may not respond immediately.
If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available.
Once it does have new information, the server immediately sends an HTTP/S response to the client, completing the open HTTP/S Request.
Upon receipt of the server response, the client can immediately issue another server request for future updates.

Based on the above considerations, we can divide our client into following four parts:

..... Internal Metadata Database will keep track of all the files, chunks, their versions, and their location in the file system.


..... Chunker will split the files into smaller pieces called chunks.
It will also be responsible for reconstructing a file from its chunks.
Our chunking algorithm will detect the parts of the files that have been modified by the user and only transfer those parts to the Cloud Storage; this will save us bandwidth and synchronization time.


..... Watcher will monitor the local workspace folders and notify the Indexer (discussed below) of any action performed by the users, e.g. when users create, delete, or update files or folders.
Watcher also listens to any changes happening on other clients that are broadcasted by Synchronization service.

..... Indexer will process the events received from the Watcher and update the internal metadata database with information about the chunks of the modified files.
Once the chunks are successfully submitted/downloaded to the Cloud Storage, the Indexer will communicate with the remote Synchronization Service to broadcast changes to other clients and update remote metadata database.

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_6.png[]

*How should clients handle slow servers?* Clients should exponentially back-off if the server is busy/not-responding.
Meaning, if a server is too slow to respond, clients should delay their retries and this delay should increase exponentially.

*Should mobile clients sync remote changes immediately?* Unlike desktop or web clients, mobile clients usually sync on demand to save user’s bandwidth and space.

*b.Metadata Database*

The Metadata Database is responsible for maintaining the versioning and metadata information about files/chunks, users, and workspaces.
The Metadata Database can be a relational database such as MySQL, or a NoSQL database service such as DynamoDB.
Regardless of the type of the database, the Synchronization Service should be able to provide a consistent view of the files using a database, especially if more than one user is working with the same file simultaneously.
Since NoSQL data stores do not support ACID properties in favor of scalability and performance, we need to incorporate the support for ACID properties programmatically in the logic of our Synchronization Service in case we opt for this kind of database.
However, using a relational database can simplify the implementation of the Synchronization Service as they natively support ACID properties.

The metadata Database should be storing information about following objects:

. Chunks
. Files
. User
. Devices
. Workspace (sync folders)


*c.Synchronization Service*

The Synchronization Service is the component that processes file updates made by a client and applies these changes to other subscribed clients.
It also synchronizes clients’ local databases with the information stored in the remote Metadata DB.
The Synchronization Service is the most important part of the system architecture due to its critical role in managing the metadata and synchronizing users’ files.
Desktop clients communicate with the Synchronization Service to either obtain updates from the Cloud Storage or send files and updates to the Cloud Storage and, potentially, other users.
If a client was offline for a period, it polls the system for new updates as soon as they come online.
When the Synchronization Service receives an update request, it checks with the Metadata Database for consistency and then proceeds with the update.
Subsequently, a notification is sent to all subscribed users or devices to report the file update.

The Synchronization Service should be designed in such a way that it transmits less data between clients and the Cloud Storage to achieve a better response time.
To meet this design goal, the Synchronization Service can employ a differencing algorithm to reduce the amount of the data that needs to be synchronized.
Instead of transmitting entire files from clients to the server or vice versa, we can just transmit the difference between two versions of a file.
Therefore, only the part of the file that has been changed is transmitted.
This also decreases bandwidth consumption and cloud data storage for the end user.
As described above, we will be dividing our files into 4MB chunks and will be transferring modified chunks only.
Server and clients can calculate a hash (e.g., SHA-256) to see whether to update the local copy of a chunk or not.
On the server, if we already have a chunk with a similar hash (even from another user), we don’t need to create another copy, we can use the same chunk.
This is discussed in detail later under Data Deduplication.

To be able to provide an efficient and scalable synchronization protocol we can consider using a communication middleware between clients and the Synchronization Service.
The messaging middleware should provide scalable message queuing and change notifications to support a high number of clients using pull or push strategies.
This way, multiple Synchronization Service instances can receive requests from a global request https://en.wikipedia.org/wiki/Message_queue[Queue], and the communication middleware will be able to balance its load.

*d.Message Queuing Service*

An important part of our architecture is a messaging middleware that should be able to handle a substantial number of requests.
A scalable Message Queuing Service that supports asynchronous message-based communication between clients and the Synchronization Service best fits the requirements of our application.
The Message Queuing Service supports asynchronous and loosely coupled message-based communication between distributed components of the system.
The Message Queuing Service should be able to efficiently store any number of messages in a highly available, reliable and scalable queue.

The Message Queuing Service will implement two types of queues in our system.
The Request Queue is a global queue and all clients will share it.
Clients’ requests to update the Metadata Database will be sent to the Request Queue first, from there the Synchronization Service will take it to update metadata.
The Response Queues that correspond to individual subscribed clients are responsible for delivering the update messages to each client.
Since a message will be deleted from the queue once received by a client, we need to create separate Response Queues for each subscribed client to share update messages.

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_6d.png[]


*e.Cloud/Block Storage*

Cloud/Block Storage stores chunks of files uploaded by the users.
Clients directly interact with the storage to send and receive objects from it.
Separation of the metadata from storage enables us to use any storage either in the cloud or in-house.

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_6e.png[title='Detailed component design for Dropbox']

=== 7.File Processing Workflow

The sequence below shows the interaction between the components of the application in a scenario when Client A updates a file that is shared with Client B and C, so they should receive the update too.
If the other clients are not online at the time of the update, the Message Queuing Service keeps the update notifications in separate response queues for them until they come online later.

. Client A uploads chunks to cloud storage.
. Client A updates metadata and commits changes.
. Client A gets confirmation and notifications are sent to Clients B and C about the changes.
. Client B and C receive metadata changes and download updated chunks.

=== 8.Data Deduplication

Data deduplication is a technique used for eliminating duplicate copies of data to improve storage utilization.
It can also be applied to network data transfers to reduce the number of bytes that must be sent.
For each new incoming chunk, we can calculate a hash of it and compare that hash with all the hashes of the existing chunks to see if we already have the same chunk present in our storage.

We can implement deduplication in two ways in our system:

.. Post-process deduplication
+
With post-process deduplication, new chunks are first stored on the storage device and later some process analyzes the data looking for duplication.
The benefit is that clients will not need to wait for the hash calculation or lookup to complete before storing the data, thereby ensuring that there is no degradation in storage performance.
Drawbacks of this approach are 1) We will unnecessarily be storing duplicate data, though for a short time, 2) Duplicate data will be transferred consuming bandwidth.

.. In-line deduplication
+
Alternatively, deduplication hash calculations can be done in real-time as the clients are entering data on their device.
If our system identifies a chunk that it has already stored, only a reference to the existing chunk will be added in the metadata, rather than a full copy of the chunk.
This approach will give us optimal network and storage usage.

=== 9.Metadata Partitioning

To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks.
We need to come up with a partitioning scheme that would divide and store our data in different DB servers.

. *Vertical Partitioning:* We can partition our database in such a way that we store tables related to one particular feature on one server.
For example, we can store all the user related tables in one database and all files/chunks related tables in another database.
Although this approach is straightforward to implement it has some issues:

.. Will we still have scale issues?
What if we have trillions of chunks to be stored and our database cannot support storing such a huge number of records?
How would we further partition such tables?
.. Joining two tables in two separate databases can cause performance and consistency issues.
How frequently do we have to join user and file tables?

. *Range Based Partitioning:* What if we store files/chunks in separate partitions based on the first letter of the File Path?
In that case, we save all the files starting with the letter ‘A’ in one partition and those that start with the letter ‘B’ into another partition and so on.
This approach is called range based partitioning.
We can even combine certain less frequently occurring letters into one database partition.
We should come up with this partitioning scheme statically so that we can always store/find a file in a predictable manner.
+
The main problem with this approach is that it can lead to unbalanced servers.
For example, if we decide to put all files starting with the letter ‘E’ into a DB partition, and later we realize that we have too many files that start with the letter ‘E’, to such an extent that we cannot fit them into one DB partition.

. *Hash-Based Partitioning:* In this scheme we take a hash of the object we are storing and based on this hash we figure out the DB partition to which this object should go.
In our case, we can take the hash of the ‘FileID’ of the File object we are storing to determine the partition the file will be stored.
Our hashing function will randomly distribute objects into different partitions, e.g., our hashing function can always map any ID to a number between [1…256], and this number would be the partition we will store our object.

This approach can still lead to overloaded partitions, which can be solved by using https://www.educative.io/courses/grokking-the-system-design-interview/B81vnyp0GpY[Consistent Hashing].

=== 10.Caching

We can have two kinds of caches in our system.
To deal with hot files/chunks we can introduce a cache for Block storage.
We can use an off-the-shelf solution like https://en.wikipedia.org/wiki/Memcached[Memcached] that can store whole chunks with its respective IDs/Hashes and Block servers before hitting Block storage can quickly check if the cache has desired chunk.
Based on clients’ usage pattern we can determine how many cache servers we need.
A high-end commercial server can have 144GB of memory; one such server can cache 36K chunks.

*Which cache replacement policy would best fit our needs?* When the cache is full, and we want to replace a chunk with a newer/hotter chunk, how would we choose?
Least Recently Used (LRU) can be a reasonable policy for our system.
Under this policy, we discard the least recently used chunk first.
Load Similarly, we can have a cache for Metadata DB.

=== 11.Load Balancer (LB)

We can add the Load balancing layer at two places in our system: 1) Between Clients and Block servers and 2) Between Clients and Metadata servers.
Initially, a simple Round Robin approach can be adopted that distributes incoming requests equally among backend servers.
This LB is simple to implement and does not introduce any overhead.
Another benefit of this approach is if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it.
A problem with Round Robin LB is, it won’t take server load into consideration.
If a server is overloaded or slow, the LB will not stop sending new requests to that server.
To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that.

=== 12.Security, Permissions and File Sharing

One of the primary concerns users will have while storing their files in the cloud is the privacy and security of their data, especially since in our system users can share their files with other users or even make them public to share it with everyone.
To handle this, we will be storing the permissions of each file in our metadata DB to reflect what files are visible or modifiable by any user.
