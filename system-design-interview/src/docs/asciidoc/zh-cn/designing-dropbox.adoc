== Designing Dropbox

让我们设计一个类似Dropbox或者Google Drive的文件主机服务。云端文件存储使用户可以将数据存储到远程服务器中。通常，这些服务器是由云端存储的提供者来维护的，并且使用户通过网路来使用服务器（通常是互联网）。用户以月付的方式为他们使用的云数据存储付费。类似的云服务还有：OneDrive、Google Drive Difficulty Level: Medium。

=== 1.Why Cloud Storage?

近来，因为云文件存储简化了多个设备之间数字资源的存储和交换，因此它变得非常流行。人们认为，从使用单个个人计算机到随时随地使用具有不同平台和操作系统的多个设备（例如智能手机和平板电脑）的转变，是云存储服务普及的原因。以下是此类云服务的主要优点：

*有效性：* 云存储的目标是使数据随时随地都可用。用户可以随时随地从任何设备上访问他们的文件或照片。

*可靠性和永久性：* 云存储的另一个优点是，它提供了100%的数据可靠性和持久性。云存储通过将数据备份到不同地区的服务器上来保证用户的数据不会丢失。

*可扩展性：* 用户不必担心数据的存储空间不足。在云存储中，只要你愿意支付相关的费用，那么你就可以无限制的存储数据。

假如在此之前你还没有使用过http://dropbox.com/[dropbox.com]，极力推荐你去创建一个账户，并且上传或编辑文件，并浏览它们提供的其他操作选项。这将有助于你理解本章内容。

=== 2.系统的要求和目标

[NOTE]
在面试开始之前，你应该始终明确你的需求。确保提出的问题是在面试者所想到的系统范围之内。

我们希望云存储系统实现什么？以下是我们系统的基本要求：

. 用户应该可以在任意设备上上传和下载他们的文件或者图片;
. 用户应该可以分享文件或者目录给其他用户；
. 我们的服务应该支持设备之间的数据自动同步，例如，在某一个设备上更新文件之后，它会被自动同步到所有的设备上;
. 系统应该支持GB级别大小的文件存储;
. 必须支持ACID，所有的文件操作应该保证原子性、一致性、隔离性和持久化;
. 我们的系统必须支持离线编辑。用户应该可以在离线状态下新增、删除和修改文件，并且一旦他们上线，他们所做的更改应该被同步到远程服务器和其他在线设备.

**扩展要求 **::
* 系统应该支持数据快照，以便用户可以回滚到文件的任意版本。

=== 3.设计注意事项

* 应该支撑超大的读写请求。 读写比应该是相近的。
* 在内部，文件可以存储在小块区域或者块中（比如4MB）；这带来很多好处，比如只需对文件更小部分重试失败的操作。如果用户上传文件失败，那么只需对失败的块重新上传文件。
* 我们可以通过传输修改后的块来减少数据交换量；
* 通过删除重复的块，可以节省存储空间和带宽；
* 在客户端保存元数据的备份（比如文件名，文件大小等），可以节省和客户端的通信次数；
* 对于较小的更改，客户端可以聪明地上传修改的部分，而不是上传整个数据块。

=== 4.容量估计和约束

* 假设总共有500,000,000用户，并且一亿日活跃用户（DAU）；
* 假设平均每个用户连接三个不同的设备；
* 平均来说，如果每个用户有200个文件或照片，那么总计有一千亿个文件；
* 假设文件的平均大小是100KB，那么总存储的大小是10千兆字节：
+
[source,text]
----
 100B * 100KB => 10PB
----

* 假设每分钟有一百万活跃用户。

=== 5.概要设计

用户会在他们的设备上指定一个文件夹作为工作空间。存储在这个文件夹中的文件、照片或者文件夹将会被上传到云存储中，并且每当一个文件被修改或者被删除时，也会以同样的方式同步到云存储中。用户可以在他们所有的设备上指定类似的工作空间，并且在一台设备上所作的任何修改都会同步到其他设备，以便在任何地方都具有相同的工作区视图。

在高层次上，我们需要存储文件及其元数据信息，比如文件名、文件大小和目录等等，以及和谁共享此文件。因此，需要一些可以帮助客户端上传到云存储或从云存储上下载文件的服务器，以及一些可以帮助更新相关文件和用户的元数据的服务器。我们还需要一些机制在发生更新时通知所有的客户段，以便他们可以同步他们的文件。

如下图所示，Block Server用于将文件上传到云存储或者从云存储中下载文件，Metadata Server将在关系型或非关系型数据库中更新文件的元数据。Synchronization Server将处理通知所有客户端同步文件的不同之处的工作流。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_5.png[title='High level design for Dropbox']

=== 6.组件设计

让我们一一介绍系统的主要组件：

*a.客户端*

客户端应用程序监控用户机器的工作空间文件夹，并且同步机器上的所有文件或文件夹到远程云存储。客户端应用程序将与存储服务器一起上传、下载和修改实际的文件到后端云存储中。客户端也与远程Synchronization Service交互以处理文件的元数据更新，比如修改文件名、文件大小、修改日期等等。

这是客户端的一些重要的操作：

. 上传和下载文件；
. 检测工作空间中更改的文件；
. 处理由离线或者并发引起的冲突。

*如何有效地处理文件传输？* 如上所述，我们可以将每个文件分成更小的块，以便我们只传输那些被修改后的块而不是整个文件。假设将每个文件分成固定大小为4MB的块。我们可以基于以下几点静态计算块的最佳大小：

. 在云存储中使用的存储设备来优化空间利用率和每秒的输入/输出操作（IOPS）；
. 网络带宽；
. 存储的文件的平均大小等。

在我们的元数据中，还应该记录每个文件和构成它的块。

*应该在客户端保存元数据的备份吗？* 保留元数据的本地副本不仅使我们能够进行离线更新，还可以节省大量往返更新远程元数据的时间。

*客户端如何有效地监听其他客户端发生的变化？* 一种解决方案是客户端定期检查服务器是否有任何更改。这种方法的问题是，当定期检测到客户端有更改，相比于服务器通知其他客户端文件有一些更改时，它们之间会有延迟。如果客户端频繁地检查服务器的变化，它不仅会浪费带宽，因为服务器在大多数时间必须返回一个空响应，而且还会使服务器处于忙碌状态。以这种方式拉取信息是不可扩展的。

上述问题的解决方案是使用HTTP长轮询。客户端使用长轮询从服务器请求信息，期望的是服务器可能不会立即进行响应。如果在收到轮询请求时服务器没有新数据，则服务器不会发送空响应，而是保持请求打开并等待响应信息可用。一旦它确定有新信息，服务器立即向客户端发送HTTP/S响应，完成打开的HTTP/S请求。收到服务器响应后，客户端可以立即发出另一个服务器请求来进一步更新数据。

基于以上思考，我们可以将客户端分为以下4个部分：

..... 内部元数据数据库将跟踪所有文件、文件存储的块、文件的版本以及它们在文件系统的位置；

..... Chunker会将文件分成更小的部分，称为块。它还负责从这些块中重建文件。分块算法将检测用户修改过的文件部分，并只将这些部分传输到云存储；这将节省我们的带宽和同步时间；

..... Watcher将会监控本地工作空间文件夹，并将用户执行的任意操作通知给Indexer（接下来讨论），例如，用户创建、删除或更新文件或文件夹。Watcher还监听同步服务广播的其他客户端发生的任意修改操作。

..... Indexer将处理来自Watcher的事件，并使用被修改的文件的块的相关信息更新内部元数据数据库。一旦块成功提交或者下载到云存储，Indexer将通知远程同步服务广播修改的信息到其他客户端并且更新远程元数据数据库。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_6.png[]

*客户端应该如何处理慢速服务器？* 如果服务器忙或者无响应，客户端应该成倍地减少访问请求。意思就是，如果一个客户端太慢而无响应，客户端应该延迟它们的重新请求的时间，并且这种延迟应该是成倍递增的。

*手机客户端应该立即同步远程的修改吗？*  和桌面或者web客户端不一样，手机客户端通常需要节省用户带宽和存储空间。


*b.Metadata数据库*

Metadata数据库负责维护文件/块、用户和工作空间的版本和元数据信息。Metadata数据库可以是关系型数据库，如MySQL，或者是非关系性数据库，如DynamoDB。不管数据库是什么类型，尤其是当多个用户同时使用同一个文件时，同步服务应该能够使用数据库提供统一的文件视图。由于NoSQL数据存储不支持ACID属性以提高可扩展性和性能，万一当我们选择这种数据库时，需要以编程的方式实现ACID属性，并将其应用到同步服务的逻辑中。但是，使用关系型数据库可以简化同步服务的实现，因为它们本身支持ACID属性。

Metadata数据库应该存储以下对象的信息：

. Chunks
. Files
. User
. Devices
. Workspace (sync folders)

*c.同步服务*

同步服务是处理客户端进行的文件更新并将这些更改应用于其他订阅的客户端的组件。
它还将客户端的本地数据库与存储在远程元数据数据库中的信息进行同步。同步服务是系统架构中最重要的部分，因为它在管理元数据和同步用户文件方面起着关键作用。桌面客户端与同步服务通信以从云存储获取更新或将文件和更新发送到云存储以及可能存在的其他用户。如果客户端离线一段时间，它会在上线后立即发起轮询请求，以获取系统中的新的更新。当同步服务收到更新请求时，它会检查Metadata数据库的一致性，然后继续更新。随后，向所有订阅的用户或设备发送通知以报告文件更新。

同步服务应该设计为在客户端和云存储之间传输更少的数据，以获取更好的响应时间。为了满足此设计目标，同步服务可以采用差分算法来减少需要同步的数据量。我们可以只传输文件的两个版本之间的差异，而不是将整个文件从客户端传输到服务器，反之亦然。因此，仅传输文件已更改的部分。这也减少了最终用户的带宽消耗和云数据存储空间。如上所述，我们将把我们的文件分成4MB大小的块，并且只传输修改过的块。服务器和客户端可以计算散列（例如，SHA-256）以查看是否更新块的本地副本。在服务器上，如果我们已经有一个具有类似哈希的块（甚至来自另一个用户），我们不需要创建另一个副本，我们可以使用相同的块。这将在后面的重复数据删除中详细讨论。

为了能够提供高效且可扩展的同步协议，我们可以考虑在客户端和同步服务之间使用通信中间件。 消息中间件应提供可扩展的消息队列和更改通知，以支持大量客户端使用拉取或推送策略。 这样，多个同步服务的实例可以接收来自全局请求 https://en.wikipedia.org/wiki/Message_queue[队列] 的请求，并且通信中间件能够对其进行负载均衡。

*d.消息队列服务*

我们的架构中一个重要的部分是应该能够处理大量请求的消息中间件。支持客户端和同步服务之间基于异步消息的通信的可扩展消息队列服务最适合我们的应用程序的要求。消息队列服务支持在系统的分布式组件之间基于消息通信的异步消息和松耦合。消息队列服务应该能够在一个高度可用、可靠和可扩展的队列中有效地存储任意数量的消息。

在系统中，消息队列服务将实现两类队列。请求队列（Request Queue）是一个全局队列，所有的客户端都使用它。更新Metadata数据库的客户端请求首先会被发送到请求队列中，然后同步服务会从队列中获取请求以更新元数据。各个订阅客户端对应的响应队列（Response Queue）负责传递更新信息到对应的客户端。由于消息一旦被客户端接收就会从队列中删除，我们需要为每个客户端创建单独的响应队列来共享更新消息。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_6d.png[]


*e.云存储/块存储*

云储存或块存储存储用户上传的文件的块。客户端直接与存储交互以从中发送和接受对象。元数据和存储的分离使我们能够使用云上或者本地的任何存储。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/dropbox/designing_dropbox_6e.png[title='Detailed component design for Dropbox']

=== 7.文件处理工作流

下面介绍客户端A更新与客户端B和客户端C共享的文件的场景中，应用程序的组件之间的交互，因此他们应该也会接受到文件的更新。如果其他客户端在文件更新时离线，消息队列服务会将更新通知保存到客户端相应的响应队列中，直到客户端上线，再将更新通知发送给客户端。

. 客户端A上传文件块到云存储；
. 客户端A更新元数据并提交修改；
. 客户端A得到一个更新成功的确认，并把文件修改的相关通知发送到客户端B和客户端C；
. 客户端B和客户端C接受修改的元数据并下载更新后的块。

=== 8.数据去重

数据去重是一种消除重复数据而提升存储空间利用率的技术。它还可以用于网络数据传输以减少必须要发送的字节数。对于每个新传入的块，我们可以计算它的哈希值，并与现存块的所有哈希值进行比较，查看我们的存储中是否已经存在相同的块。

我们的系统中实现两种去重的方式：

.. 后置处理去重 Post-process deduplication
+
在后置处理去重中，新的块首先会保存到存储设备中，随后一些数据分析处理过程会查询并处理重复的数据。这种方式的好处是，客户端存储数据时无需等待哈希值的计算或查找完成，从而保证存储性能不会下降。缺点是：1）短时间内，会存储不必要的重复数据，2）重复的数据会消耗网络带宽。

.. 在线去重
+
或者，当客户端在其设备上数据时，可以即时完成去重哈希值的计算。如果系统识别一个已经存储的块，那么只将对已存在块的引用添加到元数据中，而不是完全拷贝这个块。这种方式最大化的使用网络带宽和存储空间。

=== 9.Metadata Partitioning

To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks.
We need to come up with a partitioning scheme that would divide and store our data in different DB servers.

. *Vertical Partitioning:* We can partition our database in such a way that we store tables related to one particular feature on one server.
For example, we can store all the user related tables in one database and all files/chunks related tables in another database.
Although this approach is straightforward to implement it has some issues:

.. Will we still have scale issues?
What if we have trillions of chunks to be stored and our database cannot support storing such a huge number of records?
How would we further partition such tables?
.. Joining two tables in two separate databases can cause performance and consistency issues.
How frequently do we have to join user and file tables?

. *Range Based Partitioning:* What if we store files/chunks in separate partitions based on the first letter of the File Path?
In that case, we save all the files starting with the letter ‘A’ in one partition and those that start with the letter ‘B’ into another partition and so on.
This approach is called range based partitioning.
We can even combine certain less frequently occurring letters into one database partition.
We should come up with this partitioning scheme statically so that we can always store/find a file in a predictable manner.
+
The main problem with this approach is that it can lead to unbalanced servers.
For example, if we decide to put all files starting with the letter ‘E’ into a DB partition, and later we realize that we have too many files that start with the letter ‘E’, to such an extent that we cannot fit them into one DB partition.

. *Hash-Based Partitioning:* In this scheme we take a hash of the object we are storing and based on this hash we figure out the DB partition to which this object should go.
In our case, we can take the hash of the ‘FileID’ of the File object we are storing to determine the partition the file will be stored.
Our hashing function will randomly distribute objects into different partitions, e.g., our hashing function can always map any ID to a number between [1…256], and this number would be the partition we will store our object.

This approach can still lead to overloaded partitions, which can be solved by using https://www.educative.io/courses/grokking-the-system-design-interview/B81vnyp0GpY[Consistent Hashing].

=== 10.Caching

We can have two kinds of caches in our system.
To deal with hot files/chunks we can introduce a cache for Block storage.
We can use an off-the-shelf solution like https://en.wikipedia.org/wiki/Memcached[Memcached] that can store whole chunks with its respective IDs/Hashes and Block servers before hitting Block storage can quickly check if the cache has desired chunk.
Based on clients’ usage pattern we can determine how many cache servers we need.
A high-end commercial server can have 144GB of memory; one such server can cache 36K chunks.

*Which cache replacement policy would best fit our needs?* When the cache is full, and we want to replace a chunk with a newer/hotter chunk, how would we choose?
Least Recently Used (LRU) can be a reasonable policy for our system.
Under this policy, we discard the least recently used chunk first.
Load Similarly, we can have a cache for Metadata DB.

=== 11.Load Balancer (LB)

We can add the Load balancing layer at two places in our system: 1) Between Clients and Block servers and 2) Between Clients and Metadata servers.
Initially, a simple Round Robin approach can be adopted that distributes incoming requests equally among backend servers.
This LB is simple to implement and does not introduce any overhead.
Another benefit of this approach is if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it.
A problem with Round Robin LB is, it won’t take server load into consideration.
If a server is overloaded or slow, the LB will not stop sending new requests to that server.
To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that.

=== 12.Security, Permissions and File Sharing

One of the primary concerns users will have while storing their files in the cloud is the privacy and security of their data, especially since in our system users can share their files with other users or even make them public to share it with everyone.
To handle this, we will be storing the permissions of each file in our metadata DB to reflect what files are visible or modifiable by any user.
