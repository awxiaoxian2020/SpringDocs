== 设计Instagram

让我们设计一个类似Instagram的图片分享服务，用户可以上传图片并分享给其他用户。类似的服务有：Flickr、Picasa。难度级别：中等。

=== 1. 什么是Instagram？

Instagram是一种社交网络服务，可让其用户上传照片和视频并分享给其他用户。Instagram用户可以选择将分享的信息设置为公开的或私密的。任何公开的共享信息对其他任意用户是可见的，而私密的共享信息只能由指定的一群人访问。Instagram还允许用户把信息分享到其他社交平台，如Facebook、Twitter、Flickr和Tumblr。

为了这个练习，我们计划设计一个更简单的版本的Instagram，用户可以分享图片，同时也可以关注其他用户。每个用户的“动态消息”将包含用户关注的所有人的最新分享的照片。

=== 2. 系统需求和目标

设计Instagram时，我们将重点关注以下需求：

==== 功能型需求

1. 用户应该可以上传、下载和浏览图片；
2. 用户可以根据照片或视频的标题进行搜索；
3. 用户可以关注其他用户；
4. 系统应该生成并显示用户的动态信息，其中包含用户关注的所有人的照片。

==== 非功能性需求

1. 我们的服务需要高可用；
2. 对于动态信息的生成，系统可接受的延迟时间是200毫秒；
3. 如果一个用户一段时间内没有看到照片，系统以执行会受到影响（为了可用性）；这事可接受的。
4. 系统应该是高可靠的，任何上传的照片或者视频都不会丢失。

不在范围内：为照片添加标签、在标签上搜索照片、对照片发表评论、将用户标记到照片、关注谁等。

=== 3. 设计注意事项

系统会有大量的读请求，因此我们将专注于构建一个可以快速检索照片的系统。

. 实际上，用户可以上传任意数量的照片。在设计此系统时，有效的存储管理应该是一个关键因素；
. 浏览照片时，应该是低延迟的；
. 数据应该百分百可靠。如果用户上传一张照片，系统将保证它永远不会丢失。

=== 4. 容量预估和约束

• 假设有5亿用户，每天有1百万活跃用户； • 每天有2百万张新照片，那么每秒有23张新照片； • 照片的平均大小 => 200KB • 一天的照片所需的存储空间为
+
[source,text]
====
 2M * 200KB => 400 GB
====

• 10年需要的总存储空间为：
+
[source,text]
====
 400GB * 365 (days a year) * 10 (years) ~= 1425TB
====

=== 5. 高级设计

在高层次上，我们需要支持两种场景，一个是上传照片，另一个是浏览、搜索照片。我们的服务需要对象存储服务来存储照片，以及数据库服务器来存储照片相关的元数据信息。

=== 6. 数据库表

[NOTE]
在面试初期阶段定义数据库表会帮助了解不同组件之间的数据流，随后将会指导数据分区。

我们需要存储用户的数据，以及他们上传的照片和他们关注的用户的信息。Photo表将会存储一张照片相关的所有数据，我们需要在 `(PhotoID, CreationDate)` 上创建索引，因为我们需要获取最近上传的照片。

.Photo
[width="25%"]
|===
2+^|Photo
>s|PK|PhotoID:int|
|userID:int|
|PhotoPath:varchar(256)|
|PhotoLatitude:int|
|PhotoLongitude:int|
|UserLatitude:int|
|UserLongitude:int|
|CreationDate:datetime|
|===

.USer
[width="25%"]
|===
2+^|User
>s|PK |UserID:int|
|Name:varchar(20)|
|Email:varchar(32)|
|DateOfBirth:datetime|
|CreationDate:datetime|
|LastLogin:datetime|
|===

.UserFollow
[width="25%"]
|===
2+^|UserFollow
>s|PK >s|UserID1:int UserID2:int
|===

存储上述模式的数据的最直接的方法是使用类似MySQL的RDBMS数据库，因为我们需要使用关联查询。但是关系型数据库也带来了各种挑战，尤其是当我们需要对数据库进行扩展时。详情请参考 SQl vs NoSQL。

可以将照片存储到类似 https://en.wikipedia.org/wiki/Apache_Hadoop[HDFS] 或 https://en.wikipedia.org/wiki/Amazon_S3[S3] 的分布式存储中

我们可以将上述模式存储在分布式键值存储中，以使用NoSQL带来的便利。与照片相关的所有元数据存储在一个表中，其中的 ’键’ 是 `PhotoID`，‘值’是包含 PhotoLocation、UserLocation、CreationTimestamp等字段的对象。

我们需要存储用户和照片之间的关系信息，以便知道照片属于谁。也要存储用户关注的其他用户的信息。对于这两个表，可以存储到类似 https://en.wikipedia.org/wiki/Apache_Cassandra[Cassandra]这样的宽列数据存储中。对于 `UserPhoto` 表，‘键’是 `UserID`，‘值’是存储在不同列的属于用户的一系列 `PhotoID`。`UserFollow` 表也是类似的存储方式。

通常，Cassandra或键值存储总是维护一定数量的副本以确保可靠性。此外，在这类数据存储中，删除操作不会立即生效，在数据永久地从系统中删除之前会保留几天（以支持数据恢复）。

=== 7. 估计数据大小

让我们预估下每张表将存储多少数据，以及存储10年的数据需要多大的存储空间。

*User*：假设每一个“int”和“dateTime”是4字节，用户表中每一行数据将会是68字节：

[source,textmate]
====
 UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes
====

如果有5亿用户，将需要32GB的存储空间。

[source,text]
====
 500 million * 68 ~= 32GB
====

*Photo*：`Photo` 表中每一行数据的大小将是284字节：

[source,text]
====
 PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes
====

如果每天上传2百万张新照片，一天将需要0.5GB大小的存储空间：

[source,text]
====
 2M * 284 bytes ~= 0.5GB per day
====

10年需要1.88TB大小的存储空间。

*UserFollow*：`UserFollow` 表中每行数据将会是8字节大小。如果我们有5亿用户，平均每个用户关注500个其他的用户。那么 `UserFollow` 表将会需要1.82TB大小的存储空间：

[source,text]
====
 500 million users * 500 followers * 8 bytes ~= 1.82TB
====

所有的表存储10年的数据需要3.7TB大小的存储空间：

[source,text]
====
 32GB + 1.88TB + 1.82TB ~= 3.7TB
====

=== 8. Component Design

Photo uploads (or writes) can be slow as they have to go to the disk, whereas reads will be faster, especially if they are being served from cache.

Uploading users can consume all the available connections, as uploading is a slow process.
This means that ‘reads’ cannot be served if the system gets busy with all the write requests.
We should keep in mind that web servers have a connection limit before designing our system.
If we assume that a web server can have a maximum of 500 connections at any time, then it can’t have more than 500 concurrent uploads or reads.
To handle this bottleneck we can split reads and writes into separate services.
We will have dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the system.

Separating photos’ read and write requests will also allow us to scale and optimize each of these operations independently.

=== 9. Reliability and Redundancy

Losing files is not an option for our service.
Therefore, we will store multiple copies of each file so that if one storage server dies we can retrieve the photo from the other copy present on a different storage server.

This same principle also applies to other components of the system.
If we want to have high availability of the system, we need to have multiple replicas of services running in the system, so that if a few services die down the system still remains available and running.
Redundancy removes the single point of failure in the system.

If only one instance of a service is required to run at any point, we can run a redundant secondary copy of the service that is not serving any traffic, but it can take control after the failover when primary has a problem.

Creating redundancy in a system can remove single points of failure and provide a backup or spare functionality if needed in a crisis.
For example, if there are two instances of the same service running in production and one fails or degrades, the system can failover to the healthy copy.
Failover can happen automatically or require manual intervention.

=== 10. Data Sharding

Let’s discuss different schemes for metadata sharding:

a. Partitioning based on UserID Let’s assume we shard based on the ‘UserID’ so that we can keep all photos of a user on the same shard.
If one DB shard is 1TB, we will need four shards to store 3.7TB of data.
Let’s assume for better performance and scalability we keep 10 shards.

So we’ll find the shard number by UserID % 10 and then store the data there.
To uniquely identify any photo in our system, we can append shard number with each PhotoID.

How can we generate PhotoIDs?
Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our system.

What are the different issues with this partitioning scheme?

1. How would we handle hot users?
Several people follow such hot users and a lot of other people see any photo they upload.
2. Some users will have a lot of photos compared to others, thus making a non-uniform distribution of storage.
3. What if we cannot store all pictures of a user on one shard?
If we distribute photos of a user onto multiple shards will it cause higher latencies?
4. Storing all photos of a user on one shard can cause issues like unavailability of all of the user’s data if that shard is down or higher latency if it is serving high load etc.

b. Partitioning based on PhotoID If we can generate unique PhotoIDs first and then find a shard number through “PhotoID % 10”, the above problems will have been solved.
We would not need to append ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system.

How can we generate PhotoIDs?
Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored.
One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs.
If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field.
So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo.

Wouldn’t this key generating DB be a single point of failure?
Yes, it would be.
A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered.
For the MySQL, the following script can define such sequences:

KeyGeneratingServer1:
auto-increment-increment = 2 auto-increment-offset = 1

KeyGeneratingServer2:
auto-increment-increment = 2 auto-increment-offset = 2 We can put a load balancer in front of both of these databases to round robin between them and to deal with downtime.
Both these servers could be out of sync with one generating more keys than the other, but this will not cause any issue in our system.
We can extend this design by defining separate ID tables for Users, Photo-Comments, or other objects present in our system.

Alternately, we can implement a ‘key’ generation scheme similar to what we have discussed in Designing a URLShortening service like TinyURL.

How can we plan for the future growth of our system?
We can have a large number of logical partitions to accommodate future data growth, such that in the beginning, multiple logical partitions reside on a single physical database server.
Since each database server can have multiple database instances on it, we can have separate databases for each logical partition on any server.
So whenever we feel that a particular database server has a lot of data, we can migrate some logical partitions from it to another server.
We can maintain a config file (or a separate database) that can map our logical partitions to database servers; this will enable us to move partitions around easily.
Whenever we want to move a partition, we only have to update the config file to announce the change.

=== 11. Ranking and News Feed Generation

To create the News Feed for any given user, we need to fetch the latest, most popular and relevant photos of the people the user follows.

For simplicity, let’s assume we need to fetch top 100 photos for a user’s News Feed.
Our application server will first get a list of people the user follows and then fetch metadata info of latest 100 photos from each user.
In the final step, the server will submit all these photos to our ranking algorithm which will determine the top 100 photos (based on recency, likeness, etc.) and return them to the user.
A possible problem with this approach would be higher latency as we have to query multiple tables and perform sorting/merging/ranking on the results.
To improve the efficiency, we can pre-generate the News Feed and store it in a separate table.

Pre-generating the News Feed: We can have dedicated servers that are continuously generating users’ News Feeds and storing them in a ‘UserNewsFeed’ table.
So whenever any user needs the latest photos for their News Feed, we will simply query this table and return the results to the user.

Whenever these servers need to generate the News Feed of a user, they will first query the UserNewsFeed table to find the last time the News Feed was generated for that user.
Then, new News Feed data will be generated from that time onwards (following the steps mentioned above).

What are the different approaches for sending News Feed contents to the users?

1. Pull: Clients can pull the News Feed contents from the server on a regular basis or manually whenever they need it.
Possible problems with this approach are a) New data might not be shown to the users until clients issue a pull request b) Most of the time pull requests will result in an empty response if there is no new data.

2. Push: Servers can push new data to the users as soon as it is available.
To efficiently manage this, users have to maintain a Long Poll request with the server for receiving the updates.
A possible problem with this approach is, a user who follows a lot of people or a celebrity user who has millions of followers; in this case, the server has to push updates quite frequently.

3. Hybrid: We can adopt a hybrid approach.
We can move all the users who have a high number of follows to a pull-based model and only push data to those users who have a few hundred (or thousand) follows.
Another approach could be that the server pushes updates to all the users not more than a certain frequency, letting users with a lot of follows/updates to regularly pull data.

For a detailed discussion about News Feed generation, take a look at Designing Facebook’s Newsfeed.

=== 12. News Feed Creation with Sharded Data

One of the most important requirement to create the News Feed for any given user is to fetch the latest photos from all people the user follows.
For this, we need to have a mechanism to sort photos on their time of creation.
To efficiently do this, we can make photo creation time part of the PhotoID.
As we will have a primary index on PhotoID, it will be quite quick to find the latest PhotoIDs.

We can use epoch time for this.
Let’s say our PhotoID will have two parts; the first part will be representing epoch time and the second part will be an auto-incrementing sequence.
So to make a new PhotoID, we can take the current epoch time and append an auto-incrementing ID from our key- generating DB.
We can figure out shard number from this PhotoID ( PhotoID % 10) and store the photo there.

What could be the size of our PhotoID?
Let’s say our epoch time starts today, how many bits we would need to store the number of seconds for next 50 years?

86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds We would need 31 bits to store this number.
Since on the average, we are expecting 23 new photos per second; we can allocate 9 bits to store auto incremented sequence.
So every second we can store (2^9
=> 512) new photos.
We can reset our auto incrementing sequence every second.

We will discuss more details about this technique under ‘Data Sharding’ in Designing Twitter.

=== 13. Cache and Load balancing

Our service would need a massive-scale photo delivery system to serve the globally distributed users.
Our service should push its content closer to the user using a large number of geographically distributed photo cache servers and use CDNs (for details see Caching).

We can introduce a cache for metadata servers to cache hot database rows.
We can use Memcache to cache the data and Application servers before hitting database can quickly check if the cache has desired rows.
Least Recently Used (LRU) can be a reasonable cache eviction policy for our system.
Under this policy, we discard the least recently viewed row first.

How can we build more intelligent cache?
If we go with 80-20 rule, i.e., 20% of daily read volume for photos is generating 80% of traffic which means that certain photos are so popular that the majority of people read them.
This dictates that we can try caching 20% of daily read volume of photos and metadata.
