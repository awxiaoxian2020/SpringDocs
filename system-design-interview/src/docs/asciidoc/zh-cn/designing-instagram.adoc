== 设计Instagram

让我们设计一个类似Instagram的图片分享服务，用户可以上传图片并分享给其他用户。类似的服务有：Flickr、Picasa。难度级别：中等。

=== 1. 什么是Instagram？

Instagram是一种社交网络服务，可让其用户上传照片和视频并分享给其他用户。Instagram用户可以选择将分享的信息设置为公开的或私密的。任何公开的共享信息对其他任意用户是可见的，而私密的共享信息只能由指定的一群人访问。Instagram还允许用户把信息分享到其他社交平台，如Facebook、Twitter、Flickr和Tumblr。

为了这个练习，我们计划设计一个更简单的版本的Instagram，用户可以分享图片，同时也可以关注其他用户。每个用户的“动态消息”将包含用户关注的所有人的最新分享的照片。

=== 2. 系统需求和目标

设计Instagram时，我们将重点关注以下需求：

==== 功能型需求

1. 用户应该可以上传、下载和浏览图片；
2. 用户可以根据照片或视频的标题进行搜索；
3. 用户可以关注其他用户；
4. 系统应该生成并显示用户的动态信息，其中包含用户关注的所有人的照片。

==== 非功能性需求

1. 我们的服务需要高可用；
2. 对于动态信息的生成，系统可接受的延迟时间是200毫秒；
3. 如果一个用户一段时间内没有看到照片，系统以执行会受到影响（为了可用性）；这事可接受的。
4. 系统应该是高可靠的，任何上传的照片或者视频都不会丢失。

不在范围内：为照片添加标签、在标签上搜索照片、对照片发表评论、将用户标记到照片、关注谁等。

=== 3. 设计注意事项

系统会有大量的读请求，因此我们将专注于构建一个可以快速检索照片的系统。

. 实际上，用户可以上传任意数量的照片。在设计此系统时，有效的存储管理应该是一个关键因素；
. 浏览照片时，应该是低延迟的；
. 数据应该百分百可靠。如果用户上传一张照片，系统将保证它永远不会丢失。

=== 4. 容量预估和约束

• 假设有5亿用户，每天有1百万活跃用户； • 每天有2百万张新照片，那么每秒有23张新照片； • 照片的平均大小 => 200KB • 一天的照片所需的存储空间为
+
[source,text]
====
 2M * 200KB => 400 GB
====

• 10年需要的总存储空间为：
+
[source,text]
====
 400GB * 365 (days a year) * 10 (years) ~= 1425TB
====

=== 5. 高级设计

在高层次上，我们需要支持两种场景，一个是上传照片，另一个是浏览、搜索照片。我们的服务需要对象存储服务来存储照片，以及数据库服务器来存储照片相关的元数据信息。

=== 6. 数据库表

[NOTE]
在面试初期阶段定义数据库表会帮助了解不同组件之间的数据流，随后将会指导数据分区。

我们需要存储用户的数据，以及他们上传的照片和他们关注的用户的信息。Photo表将会存储一张照片相关的所有数据，我们需要在 `(PhotoID, CreationDate)` 上创建索引，因为我们需要获取最近上传的照片。

.Photo
[width="25%"]
|===
2+^|Photo
>s|PK|PhotoID:int|
|userID:int|
|PhotoPath:varchar(256)|
|PhotoLatitude:int|
|PhotoLongitude:int|
|UserLatitude:int|
|UserLongitude:int|
|CreationDate:datetime|
|===

.USer
[width="25%"]
|===
2+^|User
>s|PK |UserID:int|
|Name:varchar(20)|
|Email:varchar(32)|
|DateOfBirth:datetime|
|CreationDate:datetime|
|LastLogin:datetime|
|===

.UserFollow
[width="25%"]
|===
2+^|UserFollow
>s|PK >s|UserID1:int UserID2:int
|===

存储上述模式的数据的最直接的方法是使用类似MySQL的RDBMS数据库，因为我们需要使用关联查询。但是关系型数据库也带来了各种挑战，尤其是当我们需要对数据库进行扩展时。详情请参考 SQl vs NoSQL。

可以将照片存储到类似 https://en.wikipedia.org/wiki/Apache_Hadoop[HDFS] 或 https://en.wikipedia.org/wiki/Amazon_S3[S3] 的分布式存储中

我们可以将上述模式存储在分布式键值存储中，以使用NoSQL带来的便利。与照片相关的所有元数据存储在一个表中，其中的 ’键’ 是 `PhotoID`，‘值’是包含 PhotoLocation、UserLocation、CreationTimestamp等字段的对象。

我们需要存储用户和照片之间的关系信息，以便知道照片属于谁。也要存储用户关注的其他用户的信息。对于这两个表，可以存储到类似 https://en.wikipedia.org/wiki/Apache_Cassandra[Cassandra]这样的宽列数据存储中。对于 `UserPhoto` 表，‘键’是 `UserID`，‘值’是存储在不同列的属于用户的一系列 `PhotoID`。`UserFollow` 表也是类似的存储方式。

通常，Cassandra或键值存储总是维护一定数量的副本以确保可靠性。此外，在这类数据存储中，删除操作不会立即生效，在数据永久地从系统中删除之前会保留几天（以支持数据恢复）。

=== 7. 估计数据大小

让我们预估下每张表将存储多少数据，以及存储10年的数据需要多大的存储空间。

*User*：假设每一个“int”和“dateTime”是4字节，用户表中每一行数据将会是68字节：

[source,textmate]
====
 UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes
====

如果有5亿用户，将需要32GB的存储空间。

[source,text]
====
 500 million * 68 ~= 32GB
====

*Photo*：`Photo` 表中每一行数据的大小将是284字节：

[source,text]
====
 PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes
====

如果每天上传2百万张新照片，一天将需要0.5GB大小的存储空间：

[source,text]
====
 2M * 284 bytes ~= 0.5GB per day
====

10年需要1.88TB大小的存储空间。

*UserFollow*：`UserFollow` 表中每行数据将会是8字节大小。如果我们有5亿用户，平均每个用户关注500个其他的用户。那么 `UserFollow` 表将会需要1.82TB大小的存储空间：

[source,text]
====
 500 million users * 500 followers * 8 bytes ~= 1.82TB
====

所有的表存储10年的数据需要3.7TB大小的存储空间：

[source,text]
====
 32GB + 1.88TB + 1.82TB ~= 3.7TB
====

=== 8. 组件设计

因为照片是要保存到磁盘的，因此照片上传（或写入）可能会很慢，然而照片的读取将会更快，尤其是将照片保存到缓存中时。

上传照片的用户会消费掉所有可用的链接，因为上传照片是一个很慢的过程。这意味着如果系统忙于处理所有的写请求，就无法提供“读”请求。在设计系统之前，我们应该记住Web服务器是有连接限制的。如果我们假设一个Web服务器任何时间最多可以有500个连接，那么它不能有超过500个并发上传请求或者读请求。为了解决这个瓶颈问题，我们可以把读取和写入拆分为单独的服务。我们将拥有处理读请求的专用服务器和处理写请求的不同的服务器，以确保上传照片不会占用系统的全部资源。

分离照片的读写请求也将是我们能够独立的扩展和优化这两个操作中的每一个。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/instagram/desiging-instagram-8.png[]

=== 9. 可靠性和冗余

丢失文件对我们的服务来说是不可忍受的。因此，我们将会为每个文件存储多个副本，这样如果一个存储服务器死机，可以从其他存储服务器上的文件副本中检索照片。

相同的策略也适用于系统的其他组件。如果系统想要高可用，那么需要在系统中运行服务的多个副本，这样即时一些服务宕机，系统仍然可用并运行。冗余消除了系统的单点故障。

如果在任何时间都只需要运行一个服务的实例，那么可以运行该服务的一个冗余副本，该副本不提供任何服务，但是当主服务器发生故障后，它可以接管整个系统以转移故障。

在系统中创建冗余副本可以消除单点故障，并在紧机时刻为系统提供备份和备用功能。例如，如果在生成环境运行同一个服务的两个实例，并且其中一个实例出现故障或者服务降级，则系统可以进行故障转移，使用正常运行的实例。故障转移可以自动发生或者手动执行。

image::https://jcohy-resources.oss-cn-beijing.aliyuncs.com/jcohy-docs/images/system-design-interview/instagram/desiging-instagram-9.png[]

=== 10. 数据分片

接下来讨论元数据分片的不同方案：

a. *基于UserID的数据分片* 假设我们基于 `UserID` 进行分片，以便于可以把用户的所有照片保存在同一个分片上。 如果一个数据库分片的大小是1TB，存储3.7TB的数据需要4个分片。为了更好的性能和可扩展性，我们使用10个分片。
+
因此可以通过 `UserID % 10` 来计算出分区号，并把数据保存到此分区中。为了使系统中的任意照片都有唯一的身份标识，可以将分区号追加到PhotoID后面。
+
*如何生成PhotoID？* 每个数据库分区可以使PhotoID自动递增，因此我们可以将 `ShardID` 追加到 `PhotoID` 后，这将使其在整个系统中是唯一的。
+
*这个分区方案有其他的问题吗？*
+
. 如何处理热门用户？一些人会关注这样的热门用户，并且许多人会看到热门用户上传的照片；
. 相较于其他用户，有些用户会上传大量照片，因此会导致存储分布不均匀；
. 如果不能把一个用户的所有图片存储到同一个分区中该怎么办？如果将一个用户的照片分开存储在不同的设备中，访问时会造成高延迟问题吗？

b. *基于 `PhotoID` 的分区* 如果我们优先生成唯一的 `PhotoID`, 然后根据 `PhotoID % 10` 计算出分区号，那么将会解决以上的问题。这个方案中，不需要把 `ShardID` 追加到 `PhotoID` 上，因为 `PhotoID` 本身在整个系统中就是唯一的。
+
*如何生成 `PhotoID`？*
Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored.
One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs.
If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field.
So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo.
+

Wouldn’t this key generating DB be a single point of failure?
Yes, it would be.
A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered.
For the MySQL, the following script can define such sequences:
+

KeyGeneratingServer1:
auto-increment-increment = 2 auto-increment-offset = 1
+
KeyGeneratingServer2:
auto-increment-increment = 2 auto-increment-offset = 2
+
We can put a load balancer in front of both of these databases to round robin between them and to deal with downtime.
Both these servers could be out of sync with one generating more keys than the other, but this will not cause any issue in our system.
We can extend this design by defining separate ID tables for Users, Photo-Comments, or other objects present in our system.
+
Alternately, we can implement a ‘key’ generation scheme similar to what we have discussed in Designing a URLShortening service like TinyURL.

How can we plan for the future growth of our system?
We can have a large number of logical partitions to accommodate future data growth, such that in the beginning, multiple logical partitions reside on a single physical database server.
Since each database server can have multiple database instances on it, we can have separate databases for each logical partition on any server.
So whenever we feel that a particular database server has a lot of data, we can migrate some logical partitions from it to another server.
We can maintain a config file (or a separate database) that can map our logical partitions to database servers; this will enable us to move partitions around easily.
Whenever we want to move a partition, we only have to update the config file to announce the change.

=== 11. Ranking and News Feed Generation

To create the News Feed for any given user, we need to fetch the latest, most popular and relevant photos of the people the user follows.

For simplicity, let’s assume we need to fetch top 100 photos for a user’s News Feed.
Our application server will first get a list of people the user follows and then fetch metadata info of latest 100 photos from each user.
In the final step, the server will submit all these photos to our ranking algorithm which will determine the top 100 photos (based on recency, likeness, etc.) and return them to the user.
A possible problem with this approach would be higher latency as we have to query multiple tables and perform sorting/merging/ranking on the results.
To improve the efficiency, we can pre-generate the News Feed and store it in a separate table.

Pre-generating the News Feed: We can have dedicated servers that are continuously generating users’ News Feeds and storing them in a ‘UserNewsFeed’ table.
So whenever any user needs the latest photos for their News Feed, we will simply query this table and return the results to the user.

Whenever these servers need to generate the News Feed of a user, they will first query the UserNewsFeed table to find the last time the News Feed was generated for that user.
Then, new News Feed data will be generated from that time onwards (following the steps mentioned above).

What are the different approaches for sending News Feed contents to the users?

1. Pull: Clients can pull the News Feed contents from the server on a regular basis or manually whenever they need it.
Possible problems with this approach are a) New data might not be shown to the users until clients issue a pull request b) Most of the time pull requests will result in an empty response if there is no new data.

2. Push: Servers can push new data to the users as soon as it is available.
To efficiently manage this, users have to maintain a Long Poll request with the server for receiving the updates.
A possible problem with this approach is, a user who follows a lot of people or a celebrity user who has millions of followers; in this case, the server has to push updates quite frequently.

3. Hybrid: We can adopt a hybrid approach.
We can move all the users who have a high number of follows to a pull-based model and only push data to those users who have a few hundred (or thousand) follows.
Another approach could be that the server pushes updates to all the users not more than a certain frequency, letting users with a lot of follows/updates to regularly pull data.

For a detailed discussion about News Feed generation, take a look at Designing Facebook’s Newsfeed.

=== 12. News Feed Creation with Sharded Data

One of the most important requirement to create the News Feed for any given user is to fetch the latest photos from all people the user follows.
For this, we need to have a mechanism to sort photos on their time of creation.
To efficiently do this, we can make photo creation time part of the PhotoID.
As we will have a primary index on PhotoID, it will be quite quick to find the latest PhotoIDs.

We can use epoch time for this.
Let’s say our PhotoID will have two parts; the first part will be representing epoch time and the second part will be an auto-incrementing sequence.
So to make a new PhotoID, we can take the current epoch time and append an auto-incrementing ID from our key- generating DB.
We can figure out shard number from this PhotoID ( PhotoID % 10) and store the photo there.

What could be the size of our PhotoID?
Let’s say our epoch time starts today, how many bits we would need to store the number of seconds for next 50 years?

86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds We would need 31 bits to store this number.
Since on the average, we are expecting 23 new photos per second; we can allocate 9 bits to store auto incremented sequence.
So every second we can store (2^9
=> 512) new photos.
We can reset our auto incrementing sequence every second.

We will discuss more details about this technique under ‘Data Sharding’ in Designing Twitter.

=== 13. Cache and Load balancing

Our service would need a massive-scale photo delivery system to serve the globally distributed users.
Our service should push its content closer to the user using a large number of geographically distributed photo cache servers and use CDNs (for details see Caching).

We can introduce a cache for metadata servers to cache hot database rows.
We can use Memcache to cache the data and Application servers before hitting database can quickly check if the cache has desired rows.
Least Recently Used (LRU) can be a reasonable cache eviction policy for our system.
Under this policy, we discard the least recently viewed row first.

How can we build more intelligent cache?
If we go with 80-20 rule, i.e., 20% of daily read volume for photos is generating 80% of traffic which means that certain photos are so popular that the majority of people read them.
This dictates that we can try caching 20% of daily read volume of photos and metadata.
